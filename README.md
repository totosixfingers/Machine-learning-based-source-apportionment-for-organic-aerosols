##### Install dependencies

```
$ poetry install
```

##### Start a shell within the virtual environment

```
$ poetry shell
```

-----------

#### <u>Prepare data</u>

##### Transform training data from xlsx to csv

```
$ python src/classifier/train_xlsx_2_csv.py --input resources/library.xlsx --output resources/library.csv --sheet Sheet1
```

##### Transform prediction data from xlsx to csv 

```
$ python src/classifier/predict_xlsx_2_csv.py --input resources/RusanenEtAl_synthetic.xlsx --output resources/RusanenEtAl_synthetic.csv --sheet X
```

------------------

#### <u>Train the model</u>

```
$ python src/classifier/ams_sparse_logreg_train.py --csv <train_data>.csv --outdir <folder_name> --features <ints...> --penalty {l1, elasticnet} --quiet --overwritemodel
```

### Arguments

- `--csv <path>`
   Path to the CSV containing the training data
- `--outdir <folder>`
   Directory where the trained model will be saved. The filename of the model is **autogenerated**, by convention, as `model_<num_of_features>_<penalty_type>.joblib`
   *The folder will be created if it does not exist.* 
- `--features <ints...>`
   Explicit list of m/z integers (columns) to use as features.
   *There are no defaults or fallbacks; you must provide at least one.*
- `--penalty {l1, elasticnet}`
   Type of sparse regularization to apply:
  - `l1`: Lasso, performs hard feature selection.
  - `elasticnet`: Combination of L1 and L2. Requires also `--l1-ratios`.
- `--l1-ratios <floats...>` *(only with `--penalty elasticnet`)*
   Grid of `l1_ratio` values to search during tuning.
   Example: `--l1-ratios 0.5 0.75 0.9 1.0`
- `--quiet`
   Suppress scikit-learn `ConvergenceWarning` messages.
- `--overwritemodel`
   Overwrite an existing model file without asking for confirmation. If this flag is not set and the output file already exists, the script will
   prompt for confirmation before overwriting.

------

⚠️ **Note:** At least 2 samples per class are required, otherwise stratified cross-validation cannot run.



##### Examples:  

##### Using only m/z peaks:[29 30 31 43 44 55 57 60 73 82 91]

1. Using **L1** Regularization (Lasso):

   ```
   $ python src/classifier/ams_sparse_logreg_train.py --csv resources/library.csv --outdir resources/ --features 29 30 31 43 44 55 57 60 73 82 91 --penalty l1 --quiet --overwritemodel
   ```

   *(~2 mins to execute)*


   Penalizing the **absolute values** of coefficients:

   <p align="center">
   penalty = λ Σ |wᵢ|
   </p>

   Encourages sparsity → many coefficients become exactly **0**.

   

2. Using **Elastic Net** Regularization:

   ```
   $ python src/classifier/ams_sparse_logreg_train.py --csv resources/library.csv --outdir resources/ --features 29 30 31 43 44 55 57 60 73 82 91 --penalty elasticnet --l1-ratios 0.5 0.75 0.9 1.0 --quiet --overwritemodel
   ```

   *(~7 mins to execute)*

   A **mixture** of L1 and L2 penalties:

   <p align="center">
   penalty = λ [ α Σ |wᵢ| + (1 − α) Σ wᵢ² ]
   </p>

   where `α = l1_ratio`.

   Balances **sparsity (L1)** and **stability (L2)**.

   Especially useful when predictors (features) are correlated.



--------------------

#### <u>Predict with the generated model</u>

```
$ python src/classifier/ams_sparse_logreg_predict.py --csv resources/RusanenEtAl_synthetic.csv --model resources/model_11_feat.joblib --outdir resources/ 
```

----------------

##### Exit the poetry shell

```
$ exit
```



--------------------



## Regularization strength in logistic regression

In scikit-learn’s `LogisticRegression`, the regularization strength is controlled by the parameter **C**, which is the inverse of the penalty weight **λ**:

<p align="center">
λ = 1 / C
</p>

- **Small C → large λ → strong regularization**  
  Coefficients are shrunk more, and with L1 many will be forced to zero.  
- **Large C → small λ → weak regularization**  
  Coefficients are less constrained, closer to ordinary logistic regression.  

### Grid of C values in this script

For simplicity, this training script does not require the user to set λ directly.  
Instead, it searches over a fixed (*hardcoded*) grid of **C values**:

<p align="center">
C ∈ [ 10⁻³ , … , 10² ]
</p>

That is, 21 values evenly spaced in log-space between 10⁻³ and 10².  

- This covers a wide range of regularization strengths (λ = 1/C from 1000 down to 0.01).  
- The best C is chosen automatically by cross-validation.  



### Elastic Net and `--l1-ratios`

When using **Elastic Net regularization** (`--penalty elasticnet`), you must also provide a grid of **`l1_ratio` values** via the `--l1-ratios` argument:

```bash
--l1-ratios 0.5 0.75 0.9 1.0
```

So the `l1_ratio` controls the balance between L1 and L2 penalties:

* **l1_ratio = 1.0** → pure L1 (lasso): maximum sparsity, many coefficients forced to zero.
* **l1_ratio < 1.0** → mixture of L1 and L2: still sparse, but correlated features are kept more stable.

Providing a grid (e.g. `0.5 0.75 0.9 1.0`) allows cross-validation to find the best tradeoff between sparsity and stability for the dataset



✅ In short:

* The user does not need to tune λ directly — The grid of C values is hardcoded for convenience, and the script picks the best one for the data.
* For L1 penalty (`--penalty l1`), no extra arguments are needed.
* For Elastic Net penalty (`--penalty elasticnet`), the user must also pass `--l1-ratios` with a list of candidate values to test.

