# Machine Learning Source Apportionment for Organic Aerosols

This project allows training and applying ML models to apportion organic aerosol sources using AMS (Aerosol Mass Spectrometer) data.

---

### Install dependencies

```
$ poetry install
```

### Start a shell within the virtual environment

```
$ poetry shell
```

-----------

## <u>Prepare data</u>

### Transform training data from xlsx to csv

```
$ python src/classifier/train_xlsx_2_csv.py --input resources/library.xlsx --output resources/library.csv --sheet Sheet1
```

### Transform prediction data from xlsx to csv 

```
$ python src/classifier/predict_xlsx_2_csv.py --input resources/RusanenEtAl_synthetic.xlsx --output resources/RusanenEtAl_synthetic.csv --sheet X
```

------------------

## <u>Train the model</u>

```
$ python src/classifier/ams_sparse_logreg_train.py --csv <train_data>.csv --outdir <folder_name> --features <ints...> --penalty {l1, elasticnet} --quiet --overwritemodel
```

#### Training Script Arguments

- `--csv <train_data>.csv`  
  Path to the CSV file containing spectra and `label` column.

- `--outdir <output_folder>`  
  Directory to save the trained model. Created if it doesn't exist.

- `--penalty {l1, elasticnet, l2, adaptive, xgboost}`  
  Type of model to train:  
  - `l1` → Lasso logistic regression  
  - `elasticnet` → L1 + L2 mixture  
  - `l2` → Ridge logistic regression  
  - `adaptive` → Adaptive Lasso (two-step weighted L1)  
  - `xgboost` → Tree-based gradient boosting

- `--l1-ratios <floats...>` *(required if `--penalty elasticnet`)*  
  Grid of L1 ratios to try (1.0 = pure L1). Example: `--l1-ratios 0.5 0.75 0.9 1.0`

- `--overwritemodel`  
  Overwrite an existing model file without asking.

- `--quiet`  
  Suppress convergence warnings from scikit-learn.

  #### Examples

##### 1. L1 Logistic Regression
```
$ python src/classifier/ams_sparse_logreg_train.py --csv resources/library.csv --outdir resources/  --penalty l1 --quiet  --overwritemodel
```

```
$ python src/classifier/ams_sparse_logreg_train.py --csv resources/library.csv --outdir resources/ --penalty elasticnet --l1-ratios 0.5 0.75 0.9 1.0  --quiet \--overwritemodel
```

  --------------------

## <u>Predict with the generated model</u>

```
$ python src/classifier/ams_sparse_logreg_predict.py --csv resources/RusanenEtAl_synthetic.csv --model resources/model_11_feat.joblib --outdir resources/ 
```
On resources see the name of the file you produced.

----------------
## <u>Generate Normalized Class Fractions</u>

This script converts the Excel file RusanenEtAl_synthetic.xlsx (sheet 'G') into a CSV containing normalized class fractions for each source.

```
$ python src/classifier/create_fractions_csv.py --xlsx resources/RusanenEtAl_synthetic.xlsx --outdir resources/

```

#### Script Arguments

- `--xlsx <inpu_excel>.csv`  
  Path to the input Excel file containing the class data.

- `--outdir <output_folder>`  
  Output directory where the CSV with normalized fractions will be saved.
----------------

## <u>Compare True Fractions vs Predicted Probabilities</u>

This script compares the true class fractions from RusanenEtAl_synthetic_fractions.csv with predicted probabilities from the ML model output.

```
$ python src/classifier/prob_class_comparison.py  --true_csv resources/run_tests/RusanenEtAl_synthetic_fractions.csv --pred_csv resources/run_tests predictions_model_98feat_xgboost.csv --outdir resources/comparison/ --penalty xgboost

```

#### Script Arguments

- `--true_csv <inpu_excel>.csv`  
  CSV file containing the true normalized class fractions. Usually generated by create_fractions_csv.py.

- `--pred_csv <prob_csv>.csv`  
  CSV file containing predicted probabilities from the ML models.

- `--outdir <output_folder>`  
  Directory where the combined CSV and scatter plots will be saved.

- `--penalty {l1, elasticnet, l2, adaptive, xgboost}`  
  Penalty or model type label.  
----------------

## NMF and Autoencoder Source Apportionment (Additional Tools)
These scripts allow performing NMF, Autoencoder, and Source-Based Autoencoder decompositions on AMS measurement data.

### Prepare Measurement Files

Measurement input files must be in Excel (.xlsx) format with one or more of the following sheets:

- measurements or X

- Time series of AMS spectra.

  - Column 1 = timestamps

  - Columns 2..N = m/z intensities

  - Column names must contain a parseable m/z (e.g., 43, m/z 43, mz_43)

- F (optional) Ground-truth source profiles (m/z × components).

- G (optional) Ground-truth time series contributions (time × components).

These are automatically loaded using the shared Measurement class. Also it contains ploting functions.


### Run NMF

```
$ python nmf_runner.py --input <data.xlsx> --output <prefix> --k <components> --max_iter 500 --tol 1e-4
```


Arguments

-  --input
  Path to measurement file (.xlsx or .csv)

- --output
Prefix used to save learned F and G matrices

- --k
Number of components

- --max_iter
Maximum number of NMF iterations

- --tol
Convergence tolerance

Example 
```
$ python nmf_runner.py --input resources/RusanenEtAl_synthetic.xlsx --output results/nmf_run --k 5 --max_iter 600
```

This produces:

- <prefix>_F_learned.csv

- <prefix>_G_learned.csv

- Profile plots (F)

- Contribution plots (G)

- Residual plots

- Correlation vs ground truth (if available)


### Run Autoencoder / Source-Based Autoencoder



```
$ python autoencoder_runner.py --input <data.xlsx> --output <prefix> --k 5 --lr 1e-2 --epochs 500 --fixed_profiles <profiles.xlsx>

```
Arguments

- --input :
Measurement file (.xlsx or .csv)

- --output :
Prefix used for saving learned matrices

- --k :
Total number of sources (free + fixed)

- --lr :
Learning rate

- --epochs :
Number of training iterations

- --fixed_profiles :
Excel file containing multiple candidates for fixed profiles
(e.g., HOA, CCOA, BBOA).

One random variant of each is selected during training.


### Autoencoders.py (Model Definitions)


#### Autoencoder(m, k)

A linear autoencoder defined as:

- Non-negative encoder weights

- Non-negative decoder weights

Reconstruction:
- X → G → X_hat

#### SourceBasedAE(m, k, F_fixed, n_fixed)

Semi-supervised version:

- First n_fixed profiles are fixed (HOA, CCOA, BBOA, etc.)

- Remaining profiles are learned

Decoder structure:
- F = [F_fixed | F_free]
- X_hat = G @ F^T


## Exit the poetry shell

```
$ exit
```
